{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INCLUDING THE LIBRARIES NEEDS FOR THE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys, re, unicodedata, csv, unicodecsv, nltk                             \n",
    "import numpy as np\n",
    "import pycrfsuite\n",
    "\n",
    "from copy                            import deepcopy\n",
    "from collections                     import Counter\n",
    "from matplotlib                      import pyplot as plt\n",
    "from nltk                            import pos_tag, word_tokenize\n",
    "from nltk.classify                   import SklearnClassifier\n",
    "from nltk.corpus                     import stopwords\n",
    "from nltk.tag                        import CRFTagger\n",
    "from nltk.tokenize                   import RegexpTokenizer\n",
    "from nltk.stem                       import PorterStemmer, LancasterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from random                          import shuffle\n",
    "from sklearn                         import metrics\n",
    "from sklearn.feature_selection       import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics                 import confusion_matrix, precision_recall_fscore_support, classification_report\n",
    "from sklearn.model_selection         import cross_val_score\n",
    "from sklearn.svm                     import LinearSVC\n",
    "from sklearn.pipeline                import Pipeline\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING DATA AND SPLITTING DATA INTO TRAIN AND TEST\n",
    "1. TOOK THE COMMA SEPARATED FILE AS INPUT\n",
    "2. READ THE DOCUMENT LINE BY LINE\n",
    "3. PARSED LINE BY LINE \n",
    "4. SPLIT THE DOCUMENT INTO TWO SETS(TRAINING AND TEST SET)\n",
    "5. THE PERCENTAGE IS PROVIDED AS INPUT FOR SPLITTING (GENERALLY 80% CONSIDERED TRAINING DATA WHILE SPLITTING)\n",
    "6. THE PREPROCESS AND THE FEATURE CONVERSION TAKES PLACE DURING SPLITTING (BY CALLING THEIR RESPECTIVE USER DEFINED FUNCTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_data_from_file(fpath, reviewText=None):\n",
    "    with open(fpath, 'rb') as f:\n",
    "        reader = unicodecsv.reader(f, delimiter=',')\n",
    "        next(reader)\n",
    "        for line in reader:\n",
    "            (Text, Label) = parseReview(line)\n",
    "            rawData.append((Text, Label))\n",
    "    return rawData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData_1(percentage):\n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (Text, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((toFeatureVector_1(preProcess(Text)), Label))\n",
    "    for (Text, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testData.append((toFeatureVector_1(preProcess(Text)), Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData_2(percentage):\n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (Text, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((toFeatureVector_2(preProcess(Text)), Label))\n",
    "    for (Text, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testData.append((toFeatureVector_2(preProcess(Text)), Label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARSING DATA:\n",
    "\n",
    "1. SPLIT INPUT INTO FOLLOWING PARTS:\n",
    "    * TEXT\n",
    "    * LABEL\n",
    "2. THE LABEL FROM INPUT IS ASSIGNED VALUE AS PER THE REQUIRED OUTPUT CLASSES  <b>(<i>male, female</i>)</b>\n",
    "3. THE FUNCTION RETURNS A TUPLE: <b>(<i>Text, Label</i>)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseReview(reviewLine):\n",
    "    Text  = reviewLine[0]\n",
    "    Label = labelMap[reviewLine[2]]\n",
    "    return (Text, Label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING:\n",
    "\n",
    "6. CREATED TOKENS OF INDIVIDUAL WORDS USING STANDARD FUNCTION <b>(<i>nltk.word_tokenize())</i>))</b>\n",
    "6. REMOVED THE SPECIAL CHARACTERS USING <b>(<i> RegexpTokenizer(r'\\w+')</i>))</b>\n",
    "8. CONVERTED ALL TOKENS TO LOWERCASE (using regex)\n",
    "8. LEMMATIZED TOKENS USING STANDARD FUNCTION <b>(<i>nltk.stem.WordNetLemmatizer.lemmatize()</i>))</b>\n",
    "8. TRIED THE FOLLOWING STEMMING. BUT THE FEATURES REDUCED CONSIDERABLY. SO WITHDRAV MYSELF FROM USING IT. \n",
    "    *  <b>(<i>nltk.stem.PorterStemmer.stem()</i>)</b>\n",
    "    *  <b>(<i>nltk.stem.LancasterStemmer.stem()</i>)</b>\n",
    "    *  <b>(<i>nltk.stem.SnowballStemmer.stem()</i>)</b>\n",
    "8. INCLUDED THE STOPWORDS FROM CORPUS TO CHECK AND REMOVE THEM FROM TOKENS DURING LEMMATATION PROCESS \n",
    "    *  <b>(<i>nltk.corpus.stopwords.words('english')</i>))</b>\n",
    "8. CREATED TOKEN POS TAG PAIR USING <b>(<i>pos_tag()</i>))</b>\n",
    "8. BIGRAM WAS USED TO FURTHER ENHANCE THE FEATURE COUNT nltk.util import ngrams\n",
    "9. THE FUNCTION RETURNS TOKEN LIST THUS COMPLETING THE TOKENIZATION PROCESS, AND THE PREPROCESSING.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('hi', 'NNP'), ('gk', 'JJ'), ('better', 'RBR'), ('good', 'JJ'), ('get', 'VBG'), ('see', 'VB'), ('stripe', 'JJ'), ('bat', 'NNS'), ('hang', 'VBG'), ('feet', 'NNS'), ('best', 'JJS')], ['Hi this', 'this is', 'is gk', 'gk better', 'better and', 'and good', 'good getting', 'getting to', 'to see', 'see The', 'The striped', 'striped bats', 'bats are', 'are hanging', 'hanging on', 'on their', 'their feet', 'feet for', 'for best', 'Hi', 'this', 'is', 'gk', 'better', 'and', 'good', 'getting', 'to', 'see', 'The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best'])\n"
     ]
    }
   ],
   "source": [
    "def preProcess(text):\n",
    "    \n",
    "    if text == '':\n",
    "        text = \"UNK\"\n",
    "    ###=======================================###\n",
    "      ###### RegExp Remove Punctuation ###### \n",
    "    ###=======================================###\n",
    "        \n",
    "    def try_regexToken(text):\n",
    "        #print(\"inside regex\",text)\n",
    "        #should return a list of tokens\n",
    "        #word tokenisation, including punctuation removal'\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        text = tokenizer.tokenize(text)      \n",
    "        return text\n",
    "    text = try_regexToken(text)\n",
    "    regexToken = text\n",
    "    forbi = text\n",
    "        \n",
    "    ###=======================================###\n",
    "        ###### Retain Punctuation ###### \n",
    "    ###=======================================###    \n",
    "    \n",
    "    def try_tokenize(text):\n",
    "        text = word_tokenize(text)     \n",
    "        return text\n",
    "    #text = try_tokenize(text)\n",
    "    #print(\"try_regexToken\", text)    \n",
    "\n",
    "    ###=======================================###\n",
    "            ###### POS TAG ###### \n",
    "    ###=======================================###\n",
    "    \n",
    "    def try_pos_tag(text):\n",
    "        #print(\"inside postag\", text)\n",
    "        #token is parsed with pos tag \n",
    "        text = pos_tag(text)\n",
    "        return text\n",
    "    text = try_pos_tag(text)\n",
    "    #print(\"try_pos_tag\", text)\n",
    "    pos = text\n",
    "    ###=======================================###\n",
    "            ###### LOWERCASE ###### \n",
    "    ###=======================================###\n",
    "    \n",
    "    #lowercasing\n",
    "    text = [(t.lower(),p) for t,p in text]\n",
    "    #print(text)\n",
    "    \n",
    "    ###=======================================###\n",
    "            ###### STOPWORDS ###### \n",
    "    ###=======================================###\n",
    "    \n",
    "    def try_stopwords(text):\n",
    "        #stopword removal- benefits are it removes rare words\n",
    "        #print(\"inside stop\", text)\n",
    "        stop = set(stopwords.words('english'))\n",
    "        text = [(t,p) for t,p in text if t not in stop]\n",
    "        #print(\"stop\", text)\n",
    "        return text\n",
    "    text = try_stopwords(text)\n",
    "    #print(\"try_stopwords\", text)\n",
    "    stop = text\n",
    "    ###=======================================###\n",
    "            ###### LEMMATIZE ###### \n",
    "    ###=======================================###\n",
    "    \n",
    "    def try_lemmatize(text):\n",
    "        #print(\"inside lemma\", text)\n",
    "        #lemmatisation\n",
    "        lemmatiser = WordNetLemmatizer()\n",
    "        t = [(lemmatiser.lemmatize(text),pos) for text,pos in text]\n",
    "        #print(\"after\", t)\n",
    "        return text\n",
    "    text = try_lemmatize(text)\n",
    "\n",
    "    lemma = text\n",
    "    \n",
    "    ###=======================================###\n",
    "            ###### STEMMING ###### \n",
    "    ###=======================================###    \n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    lancaster = LancasterStemmer()\n",
    "    snow = SnowballStemmer(\"english\") \n",
    "    stemmed_tokens = []\n",
    "    \n",
    "    #stemming PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "    def PstemTokens(text):\n",
    "        for text,pos in text:\n",
    "            stemmed_tokens.append((porter.stem(text),pos))\n",
    "        return stemmed_tokens\n",
    "    \n",
    "    def LstemTokens(text):\n",
    "        for text,pos in text:\n",
    "            stemmed_tokens.append((lancaster.stem(text),pos))\n",
    "        return stemmed_tokens\n",
    "    \n",
    "    def SstemTokens(text):\n",
    "        for text,pos in text:\n",
    "            stemmed_tokens.append((snow.stem(text),pos))\n",
    "        return stemmed_tokens\n",
    "            \n",
    "    stemmed_tokens = PstemTokens(text)\n",
    "    #stemmed_tokens = LstemTokens(text) \n",
    "    #stemmed_tokens = SstemTokens(text) \n",
    "    text = stemmed_tokens\n",
    "    \n",
    "    bigram = [' '.join(l) for l in nltk.bigrams(forbi)] + forbi\n",
    "    tokens = text, bigram\n",
    "\n",
    "\n",
    "    #return regexToken\n",
    "    #return pos\n",
    "    #return stop\n",
    "    #return lemma\n",
    "    #return stemmed_tokens\n",
    "    return tokens\n",
    "\n",
    "t = preProcess(\"Hi, this is gk...better and good getting to see! The striped bats are hanging on their feet for best\")\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONVERTING THE TOKENS TO FEATURES\n",
    "\n",
    "1.  THE GLOBAL DICTIONARY IS FILLED WITH THE FEATURES THAT ARE ENCOUNTED DURING THE WHOLE FEATURE VECTOR CREATION PROCESS \n",
    "2.  WEIGHTED FEATURE VALUES HAVE BEEN USED FOR EACH POS TAG\n",
    "3.  THE SECOND FUNCTION USING BIGRAM TOKEN HAVE BEEN CREATED \n",
    "4.  BOTH EXECUTED INDEPENDENTLY TO CHECK THE SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDict = {} # the global feature dictionary\n",
    "\n",
    "def toFeatureVector_1(tokens):\n",
    "    featureVec = {}\n",
    "    prev_tag = {}\n",
    "    text_token = tokens[0]\n",
    "    bigram_token = tokens[1]\n",
    "    for k, pos in text_token:\n",
    "        try:\n",
    "            sum = 0\n",
    "            featureVec[k] += 1.0/len(tokens)\n",
    "            #using pos_tags\n",
    "            if k in prev_tag:\n",
    "                if pos != prev_tag[k]:\n",
    "                    featureVec[k] -= 1.0/len(tokens)\n",
    "                else:\n",
    "                    featureVec[k] += 1.0/len(tokens)\n",
    "            prev_tag[k] = pos    \n",
    "        except KeyError:\n",
    "            featureVec[k] = 1.0/len(tokens)  \n",
    "    #print(featureVec)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDict = {} # the global feature dictionary\n",
    "\n",
    "def toFeatureVector_2(tokens):\n",
    "    featureVec = {}\n",
    "    text_token = tokens[0]\n",
    "    bigram_token = tokens[1]\n",
    "    for w in bigram_token:\n",
    "        try:\n",
    "            featureVec[w] += 1.0/len(bigram_token)\n",
    "        except KeyError:\n",
    "            featureVec[w] = 1.0/len(bigram_token)\n",
    "        try:\n",
    "            featureDict[w] += 1.0/len(bigram_token)\n",
    "        except KeyError:\n",
    "            featureDict[w] = 1.0/len(bigram_token)\n",
    "    #print(featureVec)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING AND VALIDATING OUR CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainClassifier_1(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC(loss='squared_hinge', penalty='l2', random_state=0, tol=1e-04))])\n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainClassifier_2(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "\n",
    "    pipeline =  Pipeline([('tfidf', TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)),\n",
    "                          ('chi2', SelectKBest(chi2, k=1000)),\n",
    "                          ('svc', LinearSVC(loss='squared_hinge', penalty='l2', random_state=0, tol=1e-04))])    \n",
    "                          \n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: t[0], reviewSamples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CROSS VALIDATING\n",
    "\n",
    "1. THE 10 FOLD CROSS VALIDATION IS PERFORMED WHERE THE TRAINING DATA (80% OF THE WHOLE DATA) IS SPLIT IS 10 FOLDS AND EACH FOLD ACT AS TEST DATA AND THE REMAINING 10 FOLD ARE TRAINING DATA. \n",
    "2. THIS IS ACCOMPLISHED BY PUTTING THE FOLDS IN LOOP AND ROTATING THE FOLDS TO ACT AS TRAINING AND TEST DATA \n",
    "3. THUS IT FOLLOWS THE PATTERN: \n",
    "    * TRAIN DATA: [:i]  \n",
    "    * TEST DATA [i:foldSize+i] \n",
    "    * TRAIN DATA [foldSize+i:] \n",
    "    COMBINED PROPERLY IN THE CODE WITH CONCATENATION \n",
    "4. THE MAIN CODE CALLS THE CROSS VALIDATE WITH 80% OF THE MAIN DATASET AND THE NUMBER OF FOLDS AS PARAMETERS\n",
    "4. THE CLASSIFIER IS TRAINED USING THE <b>(<i> LinearSVC(), TfidfTransformer() AND SelectKBest() </i>)</b>\n",
    "5. THE TEST DATA LABELS ARE GATHERED BEFORE PREDICTING ON THE TEST DATA\n",
    "6. THE GATHERED LABELS AND PREDICTED LABELS ARE PASSED ON TO STANDARD FUNCTIONS TO IDENTIFY THE FOLLOWING:\n",
    "    * PRECISON <b>(<i>sklearn.metrics.precision_recall_fscore_support()</i>)</b>\n",
    "    * RECALL <b>(<i>sklearn.metrics.precision_recall_fscore_support()</i>)</b>\n",
    "    * F-SCORE <b>(<i>sklearn.metrics.precision_recall_fscore_support()</i>)</b>\n",
    "    * THE ABOVE VALUES ARE APPENDED TO A LIST\n",
    "7. THE AVERAGE OF EACH SCORE IS TAKEN USING MEAN \n",
    "8. FINAL RESULT IS RETURNED TO THE CALLING CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidate_1(dataset, folds):\n",
    "    shuffle(dataset)\n",
    "    results = []\n",
    "    foldSize = int(len(dataset)/folds)\n",
    "    \n",
    "    for i in range(0,len(dataset),int(foldSize)):\n",
    "        # insert code here that trains and tests on the 10 folds of data in the dataset\n",
    "        print(\"Fold start on items %d - %d\" % (i, i+foldSize))\n",
    "        myTestData = dataset[i:i+foldSize]\n",
    "        myTrainData = dataset[:i] + dataset[i+foldSize:]\n",
    "        classifier = trainClassifier_1(myTrainData)\n",
    "        y_true = [x[1] for x in myTestData]\n",
    "        y_pred = predictLabels(myTestData, classifier)\n",
    "        print(len(myTestData))\n",
    "        results.append(precision_recall_fscore_support(y_true, y_pred, average='weighted'))\n",
    "    avgResults = [np.mean([x[0] for x in results]),\n",
    "                   np.mean([x[1] for x in results]),\n",
    "                   np.mean([x[2] for x in results])\n",
    "                ]\n",
    "    return avgResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidate_2(dataset, folds):\n",
    "    shuffle(dataset)\n",
    "    results = []\n",
    "    foldSize = int(len(dataset)/folds)\n",
    "    \n",
    "    for i in range(0,len(dataset),int(foldSize)):\n",
    "        # insert code here that trains and tests on the 10 folds of data in the dataset\n",
    "        print(\"Fold start on items %d - %d\" % (i, i+foldSize))\n",
    "        myTestData = dataset[i:i+foldSize]\n",
    "        myTrainData = dataset[:i] + dataset[i+foldSize:]\n",
    "        classifier = trainClassifier_2(myTrainData)\n",
    "        y_true = [x[1] for x in myTestData]\n",
    "        y_pred = predictLabels(myTestData, classifier)\n",
    "        print(len(myTestData))\n",
    "        results.append(precision_recall_fscore_support(y_true, y_pred, average='weighted'))\n",
    "    avgResults = [np.mean([x[0] for x in results]),\n",
    "                   np.mean([x[1] for x in results]),\n",
    "                   np.mean([x[2] for x in results])\n",
    "                ]\n",
    "    return avgResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 Raw Data, 0 Train Data, 0 Test Data\n",
      "Preparing the dataset...\n",
      "Now 10112 rawData, 0 Train Data, 0 Test Data\n",
      "Preparing training and test data...\n",
      "Now 10112 rawData, 8088 Train Data, 2024 Test Data\n",
      "Preparing training and test data...\n",
      "K Fold cross-validation: \n",
      "\n",
      "\n",
      "Fold start on items 0 - 808\n",
      "Training Classifier...\n",
      "808\n",
      "Fold start on items 808 - 1616\n",
      "Training Classifier...\n",
      "808\n",
      "Fold start on items 1616 - 2424\n",
      "Training Classifier...\n",
      "808\n",
      "Fold start on items 2424 - 3232\n",
      "Training Classifier...\n",
      "808\n",
      "Fold start on items 3232 - 4040\n",
      "Training Classifier...\n",
      "808\n",
      "Fold start on items 4040 - 4848\n",
      "Training Classifier...\n",
      "808\n",
      "Fold start on items 4848 - 5656\n",
      "Training Classifier...\n",
      "808\n",
      "Fold start on items 5656 - 6464\n",
      "Training Classifier...\n",
      "808\n",
      "Fold start on items 6464 - 7272\n",
      "Training Classifier...\n",
      "808\n",
      "Fold start on items 7272 - 8080\n",
      "Training Classifier...\n",
      "808\n",
      "Fold start on items 8080 - 8888\n",
      "Training Classifier...\n",
      "8\n",
      "Fold start on items 0 - 808\n",
      "Training Classifier...\n",
      "808\n",
      "Fold start on items 808 - 1616\n",
      "Training Classifier...\n",
      "808\n",
      "Fold start on items 1616 - 2424\n",
      "Training Classifier...\n",
      "808\n",
      "Fold start on items 2424 - 3232\n",
      "Training Classifier...\n",
      "808\n",
      "Fold start on items 3232 - 4040\n",
      "Training Classifier...\n",
      "808\n",
      "Fold start on items 4040 - 4848\n",
      "Training Classifier...\n",
      "808\n",
      "Fold start on items 4848 - 5656\n",
      "Training Classifier...\n",
      "808\n",
      "Fold start on items 5656 - 6464\n",
      "Training Classifier...\n",
      "808\n",
      "Fold start on items 6464 - 7272\n",
      "Training Classifier...\n",
      "808\n",
      "Fold start on items 7272 - 8080\n",
      "Training Classifier...\n",
      "808\n",
      "Fold start on items 8080 - 8888\n",
      "Training Classifier...\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "rawData = [] # the filtered data from the dataset file (should be 21000 samples)\n",
    "trainData = [] # the training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
    "testData = [] # the test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
    "\n",
    "# the output classes\n",
    "Label1 = 'male'\n",
    "Label2 = 'female'\n",
    "labelMap = {'male' : Label1, 'female' : Label2}\n",
    "\n",
    "# references to the data files\n",
    "\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d Raw Data, %d Train Data, %d Test Data\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "\n",
    "rawData = get_raw_data_from_file(\"training.csv\") \n",
    "\n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "# You do the cross validation on the 80% (training data)\n",
    "# We print the number of training samples and the number of features before the split\n",
    "print(\"Now %d rawData, %d Train Data, %d Test Data\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "splitData_1(0.8)\n",
    "print(\"Now %d rawData, %d Train Data, %d Test Data\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "print(\"K Fold cross-validation: \")\n",
    "print('\\n')\n",
    "CV_Results_1_1 = crossValidate_1(trainData, 10)\n",
    "CV_Results_1_2 = crossValidate_2(trainData, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'alright': 0.5, 'tri': 0.5, 'get': 0.5, 'time': 0.5, 'work': 0.5}, 'male'), ({}, 'female'), ({'need': 0.5, 'wait': 0.5}, 'female'), ({'key': 0.5, 'heart': 0.5}, 'female'), ({'listen': 0.5, 'miss': 0.5, 'marpl': 0.5, 'need': 0.5, 'know': 0.5, 'gonna': 0.5, 'meet': 0.5, 'r': 1.0, 'later': 0.5, 'write': 0.5}, 'female'), ({'suppos': 0.5, 'peel': 0.5, 'spud': 0.5, 'sure': 0.5, 'find': 0.5, 'someon': 0.5, 'cover': 0.5}, 'male'), ({'believ': 0.5, 'back': 0.5}, 'female'), ({'well': 0.5, 'invit': 0.5, 'mind': 0.5}, 'male'), ({'poke': 0.5, 'around': 0.5, 'wind': 0.5}, 'female'), ({'deserv': 0.5, 'anyway': 0.5, 'brought': 0.5}, 'female')]\n",
      "Training Classifier...\n",
      "Done Testing!\n"
     ]
    }
   ],
   "source": [
    "# Finally, check the accuracy of your classifier by training on all the tranin data\n",
    "# and testing on the test set\n",
    "# Will only work once all functions are complete\n",
    "functions_complete = True  # set to True once you're happy with your methods for cross val\n",
    "if functions_complete:\n",
    "    print(testData[0:10])   # have a look at the first test data instance\n",
    "    classifier = trainClassifier_1(trainData)  # train the classifier\n",
    "    testTrue = [t[1] for t in testData]   # get the ground-truth labels from the data\n",
    "    testPred = predictLabels(testData, classifier)  # classify the test data to get predicted labels\n",
    "    POSTagging_1 = classification_report(testTrue, testPred)\n",
    "    finalScores = precision_recall_fscore_support(testTrue, testPred, average='weighted') # evaluate\n",
    "    print(\"Done Testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'alright': 0.5, 'tri': 0.5, 'get': 0.5, 'time': 0.5, 'work': 0.5}, 'male'), ({}, 'female'), ({'need': 0.5, 'wait': 0.5}, 'female'), ({'key': 0.5, 'heart': 0.5}, 'female'), ({'listen': 0.5, 'miss': 0.5, 'marpl': 0.5, 'need': 0.5, 'know': 0.5, 'gonna': 0.5, 'meet': 0.5, 'r': 1.0, 'later': 0.5, 'write': 0.5}, 'female'), ({'suppos': 0.5, 'peel': 0.5, 'spud': 0.5, 'sure': 0.5, 'find': 0.5, 'someon': 0.5, 'cover': 0.5}, 'male'), ({'believ': 0.5, 'back': 0.5}, 'female'), ({'well': 0.5, 'invit': 0.5, 'mind': 0.5}, 'male'), ({'poke': 0.5, 'around': 0.5, 'wind': 0.5}, 'female'), ({'deserv': 0.5, 'anyway': 0.5, 'brought': 0.5}, 'female')]\n",
      "Training Classifier...\n",
      "Done Testing!\n"
     ]
    }
   ],
   "source": [
    "# Finally, check the accuracy of your classifier by training on all the tranin data\n",
    "# and testing on the test set\n",
    "# Will only work once all functions are complete\n",
    "functions_complete = True  # set to True once you're happy with your methods for cross val\n",
    "if functions_complete:\n",
    "    print(testData[0:10])   # have a look at the first test data instance\n",
    "    classifier = trainClassifier_2(trainData)  # train the classifier\n",
    "    testTrue = [t[1] for t in testData]   # get the ground-truth labels from the data\n",
    "    testPred = predictLabels(testData, classifier)  # classify the test data to get predicted labels\n",
    "    POSTagging_2 = classification_report(testTrue, testPred)\n",
    "    finalScores = precision_recall_fscore_support(testTrue, testPred, average='weighted') # evaluate\n",
    "    print(\"Done Testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 10112 rawData, 8088 Train Data, 2024 Test Data\n",
      "Preparing training and test data...\n",
      "Now 10112 rawData, 16176 Train Data, 4048 Test Data\n",
      "Preparing training and test data...\n",
      "K Fold cross-validation: \n",
      "\n",
      "\n",
      "Fold start on items 0 - 1617\n",
      "Training Classifier...\n",
      "1617\n",
      "Fold start on items 1617 - 3234\n",
      "Training Classifier...\n",
      "1617\n",
      "Fold start on items 3234 - 4851\n",
      "Training Classifier...\n",
      "1617\n",
      "Fold start on items 4851 - 6468\n",
      "Training Classifier...\n",
      "1617\n",
      "Fold start on items 6468 - 8085\n",
      "Training Classifier...\n",
      "1617\n",
      "Fold start on items 8085 - 9702\n",
      "Training Classifier...\n",
      "1617\n",
      "Fold start on items 9702 - 11319\n",
      "Training Classifier...\n",
      "1617\n",
      "Fold start on items 11319 - 12936\n",
      "Training Classifier...\n",
      "1617\n",
      "Fold start on items 12936 - 14553\n",
      "Training Classifier...\n",
      "1617\n",
      "Fold start on items 14553 - 16170\n",
      "Training Classifier...\n",
      "1617\n",
      "Fold start on items 16170 - 17787\n",
      "Training Classifier...\n",
      "6\n",
      "Fold start on items 0 - 1617\n",
      "Training Classifier...\n",
      "1617\n",
      "Fold start on items 1617 - 3234\n",
      "Training Classifier...\n",
      "1617\n",
      "Fold start on items 3234 - 4851\n",
      "Training Classifier...\n",
      "1617\n",
      "Fold start on items 4851 - 6468\n",
      "Training Classifier...\n",
      "1617\n",
      "Fold start on items 6468 - 8085\n",
      "Training Classifier...\n",
      "1617\n",
      "Fold start on items 8085 - 9702\n",
      "Training Classifier...\n",
      "1617\n",
      "Fold start on items 9702 - 11319\n",
      "Training Classifier...\n",
      "1617\n",
      "Fold start on items 11319 - 12936\n",
      "Training Classifier...\n",
      "1617\n",
      "Fold start on items 12936 - 14553\n",
      "Training Classifier...\n",
      "1617\n",
      "Fold start on items 14553 - 16170\n",
      "Training Classifier...\n",
      "1617\n",
      "Fold start on items 16170 - 17787\n",
      "Training Classifier...\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(\"Now %d rawData, %d Train Data, %d Test Data\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "splitData_2(0.8)\n",
    "print(\"Now %d rawData, %d Train Data, %d Test Data\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "print(\"K Fold cross-validation: \")\n",
    "print('\\n')\n",
    "CV_Results_2_1 = crossValidate_1(trainData, 10)\n",
    "CV_Results_2_2 = crossValidate_2(trainData, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'alright': 0.5, 'tri': 0.5, 'get': 0.5, 'time': 0.5, 'work': 0.5}, 'male'), ({}, 'female'), ({'need': 0.5, 'wait': 0.5}, 'female'), ({'key': 0.5, 'heart': 0.5}, 'female'), ({'listen': 0.5, 'miss': 0.5, 'marpl': 0.5, 'need': 0.5, 'know': 0.5, 'gonna': 0.5, 'meet': 0.5, 'r': 1.0, 'later': 0.5, 'write': 0.5}, 'female'), ({'suppos': 0.5, 'peel': 0.5, 'spud': 0.5, 'sure': 0.5, 'find': 0.5, 'someon': 0.5, 'cover': 0.5}, 'male'), ({'believ': 0.5, 'back': 0.5}, 'female'), ({'well': 0.5, 'invit': 0.5, 'mind': 0.5}, 'male'), ({'poke': 0.5, 'around': 0.5, 'wind': 0.5}, 'female'), ({'deserv': 0.5, 'anyway': 0.5, 'brought': 0.5}, 'female')]\n",
      "Training Classifier...\n"
     ]
    }
   ],
   "source": [
    "# Finally, check the accuracy of your classifier by training on all the tranin data\n",
    "# and testing on the test set\n",
    "# Will only work once all functions are complete\n",
    "functions_complete = True  # set to True once you're happy with your methods for cross val\n",
    "if functions_complete:\n",
    "    print(testData[0:10])   # have a look at the first test data instance\n",
    "    classifier = trainClassifier_1(trainData)  # train the classifier\n",
    "    testTrue = [t[1] for t in testData]   # get the ground-truth labels from the data\n",
    "    testPred = predictLabels(testData, classifier)  # classify the test data to get predicted labels\n",
    "    bigram_1 = classification_report(testTrue, testPred)\n",
    "    finalScores = precision_recall_fscore_support(testTrue, testPred, average='weighted') # evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'alright': 0.5, 'tri': 0.5, 'get': 0.5, 'time': 0.5, 'work': 0.5}, 'male'), ({}, 'female'), ({'need': 0.5, 'wait': 0.5}, 'female'), ({'key': 0.5, 'heart': 0.5}, 'female'), ({'listen': 0.5, 'miss': 0.5, 'marpl': 0.5, 'need': 0.5, 'know': 0.5, 'gonna': 0.5, 'meet': 0.5, 'r': 1.0, 'later': 0.5, 'write': 0.5}, 'female'), ({'suppos': 0.5, 'peel': 0.5, 'spud': 0.5, 'sure': 0.5, 'find': 0.5, 'someon': 0.5, 'cover': 0.5}, 'male'), ({'believ': 0.5, 'back': 0.5}, 'female'), ({'well': 0.5, 'invit': 0.5, 'mind': 0.5}, 'male'), ({'poke': 0.5, 'around': 0.5, 'wind': 0.5}, 'female'), ({'deserv': 0.5, 'anyway': 0.5, 'brought': 0.5}, 'female')]\n",
      "Training Classifier...\n"
     ]
    }
   ],
   "source": [
    "# Finally, check the accuracy of your classifier by training on all the tranin data\n",
    "# and testing on the test set\n",
    "# Will only work once all functions are complete\n",
    "functions_complete = True  # set to True once you're happy with your methods for cross val\n",
    "if functions_complete:\n",
    "    print(testData[0:10])   # have a look at the first test data instance\n",
    "    classifier = trainClassifier_2(trainData)  # train the classifier\n",
    "    testTrue = [t[1] for t in testData]   # get the ground-truth labels from the data\n",
    "    testPred = predictLabels(testData, classifier)  # classify the test data to get predicted labels\n",
    "    bigram_2 = classification_report(testTrue, testPred)\n",
    "    finalScores = precision_recall_fscore_support(testTrue, testPred, average='weighted') # evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"It's no problem, honestly. Go on, go and open the launderette.  Leave it with me.\", 'female'), (\"Last night was better than ever. What's all this?  Anything interesting?\", 'male'), ('Have you checked the answerphone?  Any calls?', 'male')]\n",
      "[({'alright': 0.5, 'tri': 0.5, 'get': 0.5, 'time': 0.5, 'work': 0.5}, 'male'), ({}, 'female'), ({'need': 0.5, 'wait': 0.5}, 'female')]\n",
      "Training Classifier...\n",
      "Done Final Testing!\n"
     ]
    }
   ],
   "source": [
    "def get_raw_data_from_file(fpath, reviewText=None):\n",
    "    with open(fpath, 'rb') as f:\n",
    "        reader = unicodecsv.reader(f, delimiter=',')\n",
    "        next(reader)\n",
    "        for line in reader:\n",
    "            (Text, Label) = parseReview(line)\n",
    "            rawData.append((Text, Label))\n",
    "    return rawData\n",
    "rawData = get_raw_data_from_file(\"test.csv\")\n",
    "print(rawData[0:3])\n",
    "\n",
    "def formatData():\n",
    "    for (Text, Label) in rawData:\n",
    "        #testData.append((toFeatureVector_1(preProcess(Text)), Label))\n",
    "        testData.append((toFeatureVector_2(preProcess(Text)), Label))\n",
    "formatData()      \n",
    "print(testData[0:3])\n",
    "\n",
    "functions_complete = True  \n",
    "if functions_complete:   \n",
    "    classifier = trainClassifier_2(trainData)  \n",
    "    testTrue = [t[1] for t in testData]  \n",
    "    testPred = predictLabels(testData, classifier) \n",
    "    final = classification_report(testTrue, testPred)\n",
    "    finalScores = precision_recall_fscore_support(testTrue, testPred, average='weighted') \n",
    "    print(\"Done Final Testing!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------RESULTS OF CROSS VALIDATION - TRAINING DATA SET(80/20 SPLIT)----------------\n",
      "\n",
      "\n",
      "USING POS TAGGING ALONG WITH REGEX TOKENIZING, LEMMATIZING, STEMMING AND STOPWORDS & LINEAR SVC: \n",
      "Precision: 0.5645839500020696 , Recall: 0.5630063006300629 , F Score 0.5617520084317323\n",
      "\n",
      "\n",
      "USING POS TAGGING ALONG WITH REGEX TOKENIZING, LEMMATIZING, STEMMING AND STOPWORDS & LinearSVC, TfidfTransformer AND SelectKBest: \n",
      "Precision: 0.5796718974568399 , Recall: 0.5586183618361836 , F Score 0.5463174533342594\n",
      "\n",
      "\n",
      "USING BIGRAMS ALONG WITH REGEX TOKENIZING, LEMMATIZING, STEMMING AND STOPWORDS & LinearSVC: \n",
      "Precision: 0.596898834256199 , Recall: 0.581435880137179 , F Score 0.581367078556625\n",
      "\n",
      "\n",
      "USING BIGRAMS ALONG WITH REGEX TOKENIZING, LEMMATIZING, STEMMING AND STOPWORDS & LinearSVC, TfidfTransformer AND SelectKBest: \n",
      "Precision: 0.5780102463186285 , Recall: 0.5685331984033283 , F Score 0.5587508909586849\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print(\"------------------RESULTS OF CROSS VALIDATION - TRAINING DATA SET(80/20 SPLIT)----------------\")\n",
    "print('\\n')\n",
    "print(\"USING POS TAGGING ALONG WITH REGEX TOKENIZING, LEMMATIZING, STEMMING AND STOPWORDS & LINEAR SVC: \")\n",
    "print(\n",
    "      \"Precision:\", CV_Results_1_1[0],\n",
    "      \", Recall:\", CV_Results_1_1[1],\n",
    "      \", F Score\", CV_Results_1_1[2])\n",
    "print('\\n')\n",
    "print(\"USING POS TAGGING ALONG WITH REGEX TOKENIZING, LEMMATIZING, STEMMING AND STOPWORDS & LinearSVC, TfidfTransformer AND SelectKBest: \")\n",
    "print(\n",
    "      \"Precision:\", CV_Results_1_2[0],\n",
    "      \", Recall:\", CV_Results_1_2[1],\n",
    "      \", F Score\", CV_Results_1_2[2])\n",
    "print('\\n')\n",
    "print(\"USING BIGRAMS ALONG WITH REGEX TOKENIZING, LEMMATIZING, STEMMING AND STOPWORDS & LinearSVC: \")\n",
    "print(\n",
    "      \"Precision:\", CV_Results_2_1[0],\n",
    "      \", Recall:\", CV_Results_2_1[1],\n",
    "      \", F Score\", CV_Results_2_1[2])\n",
    "print('\\n')\n",
    "print(\"USING BIGRAMS ALONG WITH REGEX TOKENIZING, LEMMATIZING, STEMMING AND STOPWORDS & LinearSVC, TfidfTransformer AND SelectKBest: \")\n",
    "print(\n",
    "      \"Precision:\", CV_Results_2_2[0],\n",
    "      \", Recall:\", CV_Results_2_2[1],\n",
    "      \", F Score\", CV_Results_2_2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------THE CLASSIFICATION REPORT - TEST DATA SET(80/20 SPLIT)----------------\n",
      "\n",
      "\n",
      "USING POS TAGGING ALONG WITH REGEX TOKENIZING, LEMMATIZING, STEMMING AND STOPWORDS & LINEAR SVC: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.57      0.61      0.59      1017\n",
      "        male       0.58      0.53      0.55      1007\n",
      "\n",
      "    accuracy                           0.57      2024\n",
      "   macro avg       0.57      0.57      0.57      2024\n",
      "weighted avg       0.57      0.57      0.57      2024\n",
      "\n",
      "\n",
      "\n",
      "------------------THE CLASSIFICATION REPORT - TEST DATA SET(80/20 SPLIT)----------------\n",
      "\n",
      "\n",
      "USING POS TAGGING ALONG WITH REGEX TOKENIZING, LEMMATIZING, STEMMING AND STOPWORDS & LinearSVC, TfidfTransformer AND SelectKBest: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.55      0.69      0.61      1017\n",
      "        male       0.58      0.43      0.49      1007\n",
      "\n",
      "    accuracy                           0.56      2024\n",
      "   macro avg       0.57      0.56      0.55      2024\n",
      "weighted avg       0.57      0.56      0.55      2024\n",
      "\n",
      "\n",
      "\n",
      "------------------THE CLASSIFICATION REPORT - TEST DATA SET(80/20 SPLIT)----------------\n",
      "\n",
      "\n",
      "USING BIGRAMS ALONG WITH REGEX TOKENIZING, LEMMATIZING, STEMMING AND STOPWORDS & LinearSVC: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.56      0.58      0.57      2034\n",
      "        male       0.56      0.54      0.55      2014\n",
      "\n",
      "    accuracy                           0.56      4048\n",
      "   macro avg       0.56      0.56      0.56      4048\n",
      "weighted avg       0.56      0.56      0.56      4048\n",
      "\n",
      "\n",
      "\n",
      "------------------THE CLASSIFICATION REPORT - TEST DATA SET(80/20 SPLIT)----------------\n",
      "\n",
      "\n",
      "USING BIGRAMS ALONG WITH REGEX TOKENIZING, LEMMATIZING, STEMMING AND STOPWORDS & LinearSVC, TfidfTransformer AND SelectKBest: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.55      0.69      0.61      2034\n",
      "        male       0.58      0.42      0.49      2014\n",
      "\n",
      "    accuracy                           0.56      4048\n",
      "   macro avg       0.56      0.56      0.55      4048\n",
      "weighted avg       0.56      0.56      0.55      4048\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print(\"------------------THE CLASSIFICATION REPORT - TEST DATA SET(80/20 SPLIT)----------------\")\n",
    "print('\\n')\n",
    "print(\"USING POS TAGGING ALONG WITH REGEX TOKENIZING, LEMMATIZING, STEMMING AND STOPWORDS & LINEAR SVC: \")\n",
    "print(POSTagging_1)\n",
    "print('\\n')\n",
    "print(\"------------------THE CLASSIFICATION REPORT - TEST DATA SET(80/20 SPLIT)----------------\")\n",
    "print('\\n')\n",
    "print(\"USING POS TAGGING ALONG WITH REGEX TOKENIZING, LEMMATIZING, STEMMING AND STOPWORDS & LinearSVC, TfidfTransformer AND SelectKBest: \")\n",
    "print(POSTagging_2)\n",
    "print('\\n')\n",
    "print(\"------------------THE CLASSIFICATION REPORT - TEST DATA SET(80/20 SPLIT)----------------\")\n",
    "print('\\n')\n",
    "print(\"USING BIGRAMS ALONG WITH REGEX TOKENIZING, LEMMATIZING, STEMMING AND STOPWORDS & LinearSVC: \")\n",
    "print(bigram_1)\n",
    "print('\\n')\n",
    "print(\"------------------THE CLASSIFICATION REPORT - TEST DATA SET(80/20 SPLIT)----------------\")\n",
    "print('\\n')\n",
    "print(\"USING BIGRAMS ALONG WITH REGEX TOKENIZING, LEMMATIZING, STEMMING AND STOPWORDS & LinearSVC, TfidfTransformer AND SelectKBest: \")\n",
    "print(bigram_2)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------THE CLASSIFICATION REPORT - Test.csv ----------------\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.59      0.74      0.66      7591\n",
      "        male       0.66      0.50      0.57      7692\n",
      "\n",
      "    accuracy                           0.62     15283\n",
      "   macro avg       0.63      0.62      0.62     15283\n",
      "weighted avg       0.63      0.62      0.62     15283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------THE CLASSIFICATION REPORT - Test.csv ----------------\")\n",
    "print('\\n')\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| macro avg - F SCORE     |       |                  \n",
    "|:-|:-|\n",
    "|   POS TAGGING WITH LINEAR SVC|  0.57|\n",
    "|   POS TAGGING WITH LinearSVC, TfidfTransformer AND SelectKBest| 0.55  |  \n",
    "|   BIGRAM WITH LINEAR SVC| 0.56  |\n",
    "|   BIGRAM WITH LinearSVC, TfidfTransformer AND SelectKBest| 0.55 | \n",
    "|   TEST.CSV BIGRAM WITH LinearSVC, TfidfTransformer AND SelectKBest   | 0.62 |  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
